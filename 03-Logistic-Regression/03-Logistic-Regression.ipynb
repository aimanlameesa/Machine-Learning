{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Logistic Regression\n",
    "\n",
    "Thus far, the problems we've encountered have been *regression* problems, in which the target $y \\in \\mathbb{R}$.\n",
    "\n",
    "Today we'll start experimenting with *classification* problems, beginning with *binary* classification problems, in which the target $y \\in \\{ 0, 1 \\}$.\n",
    "\n",
    "## Background\n",
    "\n",
    "The simplest approach to classification, applicable when the input feature vector $\\mathbf{x} \\in \\mathbb{R}^n$, is a simple generalization of what we\n",
    "do in linear regression. Recall that in linear regression, we assume that the target is drawn from a Gaussian distribution whose mean is a linear function\n",
    "of $\\mathbf{x}$:\n",
    "\n",
    "$$ y \\sim {\\cal N}(\\theta^\\top \\mathbf{x}, \\sigma^2) $$\n",
    "\n",
    "In logistic regression, similarly, we'll assume that the target is drawn from a Bernoulli distribution with parameter $p$ being the probability of\n",
    "class 1:\n",
    "\n",
    "$$ y \\sim \\text{Bernoulli}(p) $$\n",
    "\n",
    "That's fine, but how do we model the parameter $p$? How is it related to $\\mathbf{x}$?\n",
    "\n",
    "In linear regression, we assume that the mean of the Gaussian is $\\theta^\\top \\mathbf{x}$, i.e., a linear function of $\\mathbf{x}$.\n",
    "\n",
    "In logistic regression, we'll assume that $p$ is a \"squashed\" linear function of $\\mathbf{x}$, i.e.,\n",
    "\n",
    "$$ p = \\text{sigmoid}(\\theta^\\top \\mathbf{x}) = g(\\theta^\\top \\mathbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}. $$\n",
    "\n",
    "Later, when we introduce generalized linear models, we'll see why $p$ should take this form. For now, though, we can simply note that the selection makes\n",
    "sense. Since $p$ is a discrete probability, $p$ is bounded by $0 \\le p \\le 1$. The sigmoid function $g(\\cdot)$ conveniently obeys these bounds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.arange(-5, 5, 0.1)\n",
    "plt.plot(z, sigmoid(z), 'b-')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)')\n",
    "plt.title('Logistic sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sigmoid approaches 0 as its input approaches $-\\infty$ and approaches 1 as its input approaches $+\\infty$. If its input is 0, its value is 0.5.\n",
    "\n",
    "Again, this choice of function may seem strange at this point, but bear with it! We'll derive this function from a more general principle, the generalized\n",
    "linear model, later.\n",
    "\n",
    "OK then, we now understand that for logistic regression, the assumptions are:\n",
    "\n",
    "1. The *data* are pairs $(\\textbf{x}, y) \\in \\mathbb{R}^n \\times \\{ 0, 1 \\}$.\n",
    "1. The *hypothesis function* is $h_\\theta(\\textbf{x}) = \\frac{1}{1+e^{-\\theta^\\top \\mathbf{x}}}$.\n",
    "\n",
    "What else do we need... ? A cost function and an algorithm for minimizing that cost function!\n",
    "\n",
    "## Cost function for logistic regression\n",
    "\n",
    "You can refer to the lecture notes to see the derivation, but for this lab, let's just skip to the chase.\n",
    "With the hypothesis $h_\\theta(\\mathbf{x})$ chosen as above, the log likelihood function $\\ell(\\theta)$ can be derived as\n",
    "$$ \\ell(\\theta) = \\log {\\cal L}(\\theta) =  \\sum_{i=1}^{m}y^{(i)}\\log(h_{\\theta}(\\mathbf{x}^{(i)})) + (1 - y^{(i)})\\log(1 - (h_{\\theta}(\\mathbf{x}^{(i)})) .$$\n",
    "\n",
    "Negating the log likelihood function to obtain a loss function, we have\n",
    "\n",
    "$$ J(\\theta) = - \\sum_{i=1}^m y^{(i)}\\log h_\\theta(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1-h_\\theta(\\textbf{x}^{(i)})) .$$\n",
    "\n",
    "There is no closed-form solution to this problem like there is in linear regression, so we have to use gradient descent to find $\\theta$ minimizing $J(\\theta)$.\n",
    "Luckily, the function *is* convex in $\\theta$ so there is just a single global minimum, and gradient descent is guaranteed to get us there eventually if we take\n",
    "the right step size.\n",
    "\n",
    "The *stochastic* gradient of $J$, for a single observed pair $(\\mathbf{x}, y)$, turns out to be (see lecture notes)\n",
    "\n",
    "$$\\nabla_J(\\theta) = (h_\\theta(\\mathbf{x}) - y)\\mathbf{x} .$$\n",
    "\n",
    "Give some thought as to whether following this gradient to increase the loss $J$ would make a worse classifier, and vice versa!\n",
    "\n",
    "Finally, we obtain the update rule for the $j^{th}$ iteration selecting training pattern $i$:\n",
    "\n",
    "$$ \\theta^{(j+1)} \\leftarrow \\theta^{(j)} + \\alpha(y^{(i)} - h_\\theta(\\textbf{x}^{(i)}))\\textbf{x}^{(i)} .$$ \n",
    "\n",
    "Note that we can perform *batch gradient descent* simply by summing the single-pair gradient over the entire training set before taking a step,\n",
    "or *mini-batch gradient descent* by summing over a small subset of the data.\n",
    "\n",
    "## Example dataset 1: student admissions data\n",
    "\n",
    "This example is from Andrew Ng's machine learning course on Coursera.\n",
    "\n",
    "The data contain students' scores for two standardized tests and an admission decision (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student admissions data.\n",
    "data = np.loadtxt('ex2data1.txt',delimiter = ',')\n",
    "exam1_data = data[:,0]\n",
    "exam2_data = data[:,1]\n",
    "X = np.array([exam1_data, exam2_data]).T\n",
    "y = data[:,2]\n",
    "\n",
    "# Output some sample data\n",
    "\n",
    "print('Exam scores', X[0:5,:])\n",
    "print('-----------------------------')\n",
    "print('Admission decision', y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "idx_0 = np.where(y == 0)\n",
    "idx_1 = np.where(y == 1)\n",
    "\n",
    "fig1 = plt.figure(figsize=(5, 5)) \n",
    "ax = plt.axes()\n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(exam1_data[idx_0], exam2_data[idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(exam1_data[idx_1], exam2_data[idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find good values for $\\theta$ without normalizing the data.\n",
    "We will definitely want to split the data into train and test, however..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(12)\n",
    "\n",
    "# Partion data into training and test datasets\n",
    "m, n = X.shape\n",
    "XX = np.insert(X, 0, 1, axis=1)\n",
    "y = y.reshape(m, 1)\n",
    "idx = np.arange(0, m)\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(m * percent_train)\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All important functions are here\n",
    "- Sigmoid function\n",
    "- Hypothesis function\n",
    "- Gradient function\n",
    "- Cost $j$ and gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):   \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def h(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def grad_j(X, y, y_pred):\n",
    "    return X.T @ (y - y_pred) / X.shape[0]\n",
    "    \n",
    "def j(theta, X, y):    \n",
    "    y_pred = h(X, theta)\n",
    "    error = (-y * np.log(y_pred)) - ((1 - y) * np.log(1 - y_pred))\n",
    "    cost = sum(error) / X.shape[0]\n",
    "    grad = grad_j(X, y, y_pred)\n",
    "    return cost[0], grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a feel for how h works\n",
    "theta_initial = np.zeros((n+1, 1))\n",
    "\n",
    "print('Initial theta:', theta_initial)\n",
    "print('Initial predictions:', h(XX, theta_initial)[0:5,:])\n",
    "print('Targets:', y[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch training function for num_iters iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, theta_initial, alpha, num_iters):\n",
    "    theta = theta_initial\n",
    "    j_history = []\n",
    "    for i in range(num_iters):\n",
    "        cost, grad = j(theta, X, y)\n",
    "        theta = theta + alpha * grad\n",
    "        j_history.append(cost)\n",
    "    return theta, j_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 3000 iterations on full training set\n",
    "alpha = .0005\n",
    "num_iters = 1000000\n",
    "theta, j_history = train(X_train, y_train, theta_initial, alpha, num_iters)\n",
    "\n",
    "print(\"Theta optimized:\", theta)\n",
    "print(\"Cost with optimized theta:\", j_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(j_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\theta)$\")\n",
    "plt.title(\"Training cost over time with batch gradient descent (no normalization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "576e44c3b1569c17fef6f70f258d4cec",
     "grade": false,
     "grade_id": "cell-84bc62ed117ee241",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In-lab exercise from example 1 (Total 35 points)\n",
    "\n",
    "That took a long time, right?\n",
    "\n",
    "See if you can do better. \n",
    "\n",
    "1. Try increasing the learning rate $\\alpha$ and starting with a better initial $\\theta$. How much does it help?\n",
    "   - Try at least 2 learning rate $\\alpha$ with 2 difference $\\theta$ (4 experiments)\n",
    "   - Do not forget to plot the graph to compare youre results\n",
    "\n",
    "2. Better yet, try *normalizing the data* and see if the training converges better. How did it go? \n",
    "   - Do not forget to plot the graph to compare youre results between unnormalized and normalized data.\n",
    "\n",
    "3. Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 (5 points)\n",
    "\n",
    "Fill $\\alpha$ and $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fea93a8eacff9b7b1d1db9098abbb952",
     "grade": false,
     "grade_id": "cell-98f6eb46f41c6686",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None' value to number(s) or function\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# declare your alphas\n",
    "alpha1 = None\n",
    "alpha2 = None\n",
    "\n",
    "# initialize thetas as you want\n",
    "theta_initial1 = None\n",
    "theta_initial2 = None\n",
    "\n",
    "# define your num iterations\n",
    "num_iters = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "053e98ba0c27eb56eadb89857573b428",
     "grade": true,
     "grade_id": "cell-f9ecb4ab4402c8b4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_list = [alpha1, alpha2]\n",
    "print('alpha 1:', alpha1)\n",
    "print('alpha 2:', alpha2)\n",
    "\n",
    "theta_initial_list = [theta_initial1, theta_initial2]\n",
    "print('theta 1:', theta_initial_list[0])\n",
    "print('theta 2:', theta_initial_list[1])\n",
    "\n",
    "print('Use num iterations:', num_iters)\n",
    "\n",
    "# Test function: Do not remove\n",
    "assert alpha_list[0] is not None and alpha_list[1] is not None, \"Alpha has not been filled\"\n",
    "chk1 = isinstance(alpha_list[0], (int, float))\n",
    "chk2 = isinstance(alpha_list[1], (int, float))\n",
    "assert chk1 and chk2, \"Alpha must be number\"\n",
    "assert theta_initial_list[0] is not None and theta_initial_list[1] is not None, \"initialized theta has not been filled\"\n",
    "chk1 = isinstance(theta_initial_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(theta_initial_list[1], (list,np.ndarray))\n",
    "assert chk1 and chk2, \"Theta must be list\"\n",
    "chk1 = ((n+1, 1) == theta_initial_list[0].shape)\n",
    "chk2 = ((n+1, 1) == theta_initial_list[1].shape)\n",
    "assert chk1 and chk2, \"Theta size are incorrect\"\n",
    "assert num_iters is not None and isinstance(num_iters, int), \"num_iters must be integer\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 (5 points)\n",
    "\n",
    "Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9444a50fd763a5d22de0cc2d21c99107",
     "grade": false,
     "grade_id": "cell-77a540a2a0cc2031",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grade task: change 'None, None' value to number(s) or function\n",
    "j_history_list = []\n",
    "theta_list = []\n",
    "for alpha in alpha_list:\n",
    "    for theta_initial in theta_initial_list:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        theta_i, j_history_i = None, None\n",
    "        j_history_list.append(j_history_i)\n",
    "        theta_list.append(theta_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d8b32e551ac03e7e946f2c677e92d0",
     "grade": true,
     "grade_id": "cell-57627ff7e32cd714",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test function: Do not remove\n",
    "assert theta_list[0] is not None and j_history_list[0] is not None, \"No values in theta_list or j_history_list\"\n",
    "chk1 = isinstance(theta_list[0], (list,np.ndarray))\n",
    "chk2 = isinstance(j_history_list[0][0], (int, float))\n",
    "assert chk1 and chk2, \"Wrong type in theta_list or j_history_list\"\n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3 (10 points)\n",
    "\n",
    "Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f63a65931d5f3da4e50c7cd36990e0f",
     "grade": true,
     "grade_id": "cell-33ed3657769adb04",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a43fae10e518b1ce4063a6991fa66ffc",
     "grade": false,
     "grade_id": "cell-59c862747f0fb871",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.4 (10 points)\n",
    "\n",
    "- Repeat your training, but **normalized data** before run training\n",
    "- Compare the results between **normalized data** and **unnormalized data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1cd115f866825c9970f827ee43d67f",
     "grade": false,
     "grade_id": "cell-43c3a360c938e4be",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1.5 (5 points)\n",
    "\n",
    "Discuss the effects of normalization, learning rate, and initial $\\theta$ in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary\n",
    "\n",
    "Note that when $\\theta^\\top \\textbf{x} = 0$, we have $h_\\theta(\\textbf{x}) = 0.5$. That is, we are\n",
    "equally unsure as to whether $\\textbf{x}$ belongs to class 0 or class 1. The contour at which\n",
    "$h_\\theta(\\textbf{x}) = 0.5$ is called the classifier's *decision boundary*.\n",
    "\n",
    "We know that in the plane, the equation $$ax+by+c=0$$ is the general form of a 2D line. In our case, we have\n",
    "$$\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0$$ as our decision boundary, but clearly, this is just a 2D line\n",
    "in the plane. So when we plot $x_1$ against $x_2$, it is easy to plot the boundary line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_points(X, theta):\n",
    "    v_orthogonal = np.array([[theta[1,0]],[theta[2,0]]])\n",
    "    v_ortho_length = np.sqrt(v_orthogonal.T @ v_orthogonal)\n",
    "    dist_ortho = theta[0,0] / v_ortho_length\n",
    "    v_orthogonal = v_orthogonal / v_ortho_length\n",
    "    v_parallel = np.array([[-v_orthogonal[1,0]],[v_orthogonal[0,0]]])\n",
    "    projections = X @ v_parallel\n",
    "    proj_1 = min(projections)\n",
    "    proj_2 = max(projections)\n",
    "    point_1 = proj_1 * v_parallel - dist_ortho * v_orthogonal\n",
    "    point_2 = proj_2 * v_parallel - dist_ortho * v_orthogonal\n",
    "    return point_1, point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(5,5)) \n",
    "ax = plt.axes() \n",
    "ax.set_aspect(aspect = 'equal', adjustable = 'box')\n",
    "plt.title('Logistic regression boundary')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.grid(axis='both', alpha=.25)\n",
    "ax.scatter(X[:,0][idx_0], X[:,1][idx_0], s=50, c='r', marker='*', label='Not Admitted')\n",
    "ax.scatter(X[:,0][idx_1], X[:,1][idx_1], s=50, c='b', marker='o', label='Admitted')\n",
    "point_1, point_2 = boundary_points(X, theta)\n",
    "plt.plot([point_1[0,0], point_2[0,0]],[point_1[1,0], point_2[1,0]], 'g-')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You'll have to adjust the above code to make it work with normalized data.\n",
    "\n",
    "### Test set performance\n",
    "\n",
    "Now let's apply the learned classifier to the test data we reserved in the beginning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y, y_pred):\n",
    "    return 1 - np.square(y - y_pred).sum() / np.square(y - y.mean()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_soft = h(X_test, theta)\n",
    "y_test_pred_hard = (y_test_pred_soft > 0.5).astype(int)\n",
    "\n",
    "test_rsq_soft = r_squared(y_test, y_test_pred_soft)\n",
    "test_rsq_hard = r_squared(y_test, y_test_pred_hard)\n",
    "test_acc = (y_test_pred_hard == y_test).astype(int).sum() / y_test.shape[0]\n",
    "\n",
    "print('Got test set soft R^2 %0.4f, hard R^2 %0.4f, accuracy %0.2f' % (test_rsq_soft, test_rsq_hard, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, accuracy is probably the more useful measure of goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Loan prediction dataset\n",
    "\n",
    "Let's take another example dataset and see what we can do with it.\n",
    "\n",
    "This dataset is from [Kaggle](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset).\n",
    "\n",
    "The data concern loan applications. It has 12 independent variables, including 5 categorical variables. The dependent variable is the decision \"Yes\" or \"No\" for extending a loan to an individual who applied.\n",
    "\n",
    "One thing we will have to do is to clean the data, by filling in missing values and converting categorical data to reals.\n",
    "We will use the Python libraries pandas and sklearn to help with the data cleaning and preparation.\n",
    "\n",
    "### Read the data and take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas. You may need to run \"pip3 install pandas\" at the console if it's not already installed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import the data\n",
    "\n",
    "data_train = pd.read_csv('train_LoanPrediction.csv')\n",
    "data_test = pd.read_csv('test_LoanPrediction.csv')\n",
    "\n",
    "# Start to explore the data\n",
    "\n",
    "print('Training data shape', data_train.shape)\n",
    "print('Test data shape', data_test.shape)\n",
    "\n",
    "print('Training data:\\n', data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training and test data\n",
    "\n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())\n",
    "print('Missing values for test data \\n ------------------------\\n', data_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values\n",
    "\n",
    "We can see from the above table that the `Married` column has 3 missing values in the training dataset and 0 missing values in the test dataset.\n",
    "Let's take a look at the distribution over the datasets then fill in the missing values in approximately the same ratio.\n",
    "\n",
    "You may be interested to look at the [documentation of the Pandas `fillna()` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html). It's great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ratio of each category value\n",
    "# Divide the missing values based on ratio\n",
    "# Fillin the missing values\n",
    "# Print the values before and after filling the missing values for confirmation\n",
    "\n",
    "print(data_train['Married'].value_counts())\n",
    "\n",
    "married = data_train['Married'].value_counts()\n",
    "print('Elements in Married variable', married.shape)\n",
    "print('Married ratio ', married[0]/sum(married.values))\n",
    "\n",
    "def fill_martial_status(data, yes_num_train, no_num_train):        \n",
    "    data['Married'].fillna('Yes', inplace = True, limit = yes_num_train)\n",
    "    data['Married'].fillna('No', inplace = True, limit = no_num_train)  \n",
    "\n",
    "fill_martial_status(data_train, 2, 1)\n",
    "print(data_train['Married'].value_counts()) \n",
    "print('Missing values for train data:\\n------------------------\\n', data_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of examples missing the `Married` attribute is 0.\n",
    "\n",
    "Excercise: Complete the data processing based on examples given and logistic regression model on training dataset. Estimate the Accuracy (goodness of fit) on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example of filling in missing values for the \"number of dependents\" attribute.\n",
    "# Here we see that categorical values are all numeric except one value \"3+\" \n",
    "# Create a new category value \"4\" for \"3+\" and ensure that all the data is numeric\n",
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "dependent = data_train['Dependents'].value_counts()\n",
    "\n",
    "print('Dependent ratio 1 ', dependent['0'] / sum(dependent.values))\n",
    "print('Dependent ratio 2 ', dependent['1'] / sum(dependent.values))\n",
    "print('Dependent ratio 3 ', dependent['2'] / sum(dependent.values))\n",
    "print('Dependent ratio 3+ ', dependent['3+'] / sum(dependent.values))\n",
    "\n",
    "def fill_dependent_status(num_0_train, num_1_train, num_2_train, num_3_train, num_0_test, num_1_test, num_2_test, num_3_test):        \n",
    "    data_train['Dependents'].fillna('0', inplace=True, limit = num_0_train)\n",
    "    data_train['Dependents'].fillna('1', inplace=True, limit = num_1_train)\n",
    "    data_train['Dependents'].fillna('2', inplace=True, limit = num_2_train)\n",
    "    data_train['Dependents'].fillna('3+', inplace=True, limit = num_3_train)\n",
    "    data_test['Dependents'].fillna('0', inplace=True, limit = num_0_test)\n",
    "    data_test['Dependents'].fillna('1', inplace=True, limit = num_1_test)\n",
    "    data_test['Dependents'].fillna('2', inplace=True, limit = num_2_test)\n",
    "    data_test['Dependents'].fillna('3+', inplace=True, limit = num_3_test)\n",
    "\n",
    "fill_dependent_status(9, 2, 2, 2, 5, 2, 2, 1)\n",
    "\n",
    "print(data_train['Dependents'].value_counts())\n",
    "\n",
    "# Convert category value \"3+\" to \"4\"\n",
    "\n",
    "data_train['Dependents'].replace('3+', 4, inplace = True)\n",
    "data_test['Dependents'].replace('3+', 4, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once missing values are filled in, you'll want to convert strings to numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here's an example of replacing missing values for a numeric attribute. Typically, we would use the mean of the attribute over the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['LoanAmount'].value_counts())\n",
    "\n",
    "LoanAmt = data_train['LoanAmount'].value_counts()\n",
    "\n",
    "print('mean loan amount ', np.mean(data_train[\"LoanAmount\"]))\n",
    "\n",
    "loan_amount_mean = np.mean(data_train[\"LoanAmount\"])\n",
    "\n",
    "data_train['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 22)\n",
    "data_test['LoanAmount'].fillna(loan_amount_mean, inplace=True, limit = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1288cf84e0d61caa8f2a40e2c323f327",
     "grade": false,
     "grade_id": "cell-2e5822ee2e432273",
     "locked": true,
     "points": 65,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Take-home exercise (65 points)\n",
    "\n",
    "Using the data from Example 2 above, finish the data cleaning and\n",
    "preparation. Build a logistic regression model based on the\n",
    "cleaned dataset and report the accuracy on the test and training sets.\n",
    "\n",
    "- Setup X and Y data (10 points)\n",
    "- Train data and return theta and J value. Find the good $\\alpha$ and you may normalized data before train. (30 points)\n",
    "- Use $\\theta$ and implement in test set. (10 points)\n",
    "- Summarize what did you do and how to find the best result in this take home exercise. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To turn in\n",
    "\n",
    "Turn in a brief report in the form of a Jupyter notebook explaining what\n",
    "you did for the in-lab exercise and the take-home exercise. Discuss what\n",
    "you learned in terms of normalization and data cleaning and the results\n",
    "you obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
